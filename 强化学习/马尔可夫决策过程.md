#  马尔可夫过程

#### 马尔可夫过程是一种无记忆的随机过程

#### 马尔可夫过程可以分为3类：

​	**1.时间，状态都离散的马尔可夫过程（马尔可夫链）**

![](.\马尔可夫img\20250730-183429.png)



​	**2.时间连续，状态离散的马尔可夫过程（连续时间的马尔可夫链）**

​	**3.时间，状态都连续的马尔可夫过程**

#### 定义

 ![20250730-183143.png](.\马尔可夫img\20250730-183143.png)

## 马尔可夫性质

**未来的状态只与现在有关，与过去无关**

![20250730-182213.png](.\马尔可夫img\20250730-182213.png)

## 状态

![20250730-182417.png](.\马尔可夫img\20250730-182417.png)

*其中 s 表示的是当前状态，s‘ 表示的是未来的状态*

## 转移矩阵

 

![](.\马尔可夫img\20250730-182650.png)

p11表示状态1转移到状态1；p1n表示状态1转移到状态n



## 分幕

 **从马尔可夫链中采样一些序列，每个序列又称为**幕****

eg:

马尔可夫链：

![](.\马尔可夫img\20250730-183648.png)



幕：

![](.\马尔可夫img\20250730-183745.png)

# 马尔可夫  **奖励**  过程

**马尔可夫奖励过程是具有价值的马尔可夫链**

![](.\马尔可夫img\20250730-184032.png)



## 奖励

eg:

![20250801-093229.png](.\马尔可夫img\20250801-093229.png)

**图中的奖励函数Rs中，在St时刻的奖励是在此时刻做出动作后由下一个时刻生成的，所以使用了Rt+1来表示**

## 折扣因子

## 回报

![20250801-093619.png](.\马尔可夫img\20250801-093619.png)

**γ的作用如下：**

![20250801-093751.png](.\马尔可夫img\20250801-093751.png)

![20250801-094133.png](.\马尔可夫img\20250801-094133.png)



## 价值函数

**价值函数是强化学习的核心概念**

**价值函数v(s)给出状态s的长期价值**

**价值函数输入为某个状态，输出为这个状态的价值**

**价值函数的定义：**

在马尔可夫奖励过程中，一个状态的**期望回报**被称为这个状态的价值函数

![20250801-094603.png](.\马尔可夫img\20250801-094603.png)

其中的求期望消除了两方面的随机性：

​	1.在状态s情况转移到另一个状态的随机性

​	2.状态转移时，返回奖励大小的随机性

*回报与价值的比较：*

**回报**定义：在一个马尔可夫奖励过程中，从t时刻的状态St开始，直到终止状态时，**所有奖励的衰减之**和Gt称为**回报**

![20250801-095353.png](.\马尔可夫img\20250801-095353.png)

**价值函数：是对回报的期望，去除里面的随机性**



## 贝尔曼方程

**贝尔曼方程，可以求解价值函数**

![20250801-101605.png](.\马尔可夫img\20250801-101605.png)

贝尔曼方程定义了St与St+1之间的关系，St是当前的状态，St+1是将来的状态，Rt+1是将来的奖励

价值函数可分解为两部分：

![20250801-101921.png](.\马尔可夫img\20250801-101921.png)

 ![20250801-102023.png](.\马尔可夫img\20250801-102225.png)

![20250801-103241.png](.\马尔可夫img\20250801-103241.png)



![20250801-110232.png](.\马尔可夫img\20250801-110232.png)

![20250801-110612.png](.\马尔可夫img\20250801-110612.png)



![20250801-110819.png](.\马尔可夫img\20250801-110819.png)





# 马尔可夫  **决策**  过程

**马尔可夫决策过程（MDP）是具有决策的马尔可夫奖励过程（MRP），其所有状态满足马尔可夫性质**

![20250801-111302.png)](.\马尔可夫img\20250801-111302.png)



## 动作

![20250801-112917.png](.\马尔可夫img\20250801-112917.png)

## 策略

![20250801-113004.png](.\马尔可夫img\20250801-113004.png)

**策略完全定义了智能体的行为**

**马尔可夫决策过程中，策略仅取决于当前状态（而不是历史记录）**

解释：在s状态下做a动作的概率为策略

![20250801-181404.png)](.\马尔可夫img\20250801-181404.png)



## 状态价值函数

![20250801-183603.png](.\马尔可夫img\20250801-183603.png)



## 动作价值函数

![20250801-183733.png](.\马尔可夫img\20250801-183733.png)

注意区分：动作价值函数和状态价值函数的区别；在于状态价值函数包含了所有的动作，见下面的公式推导

## ***贝尔曼期望方程***

**状态函数可以分解为：**即时奖励+后继状态的后继状态的折扣值

![20250801-184131.png](.\马尔可夫img\20250801-184131.png)

**动作价值函数可进行类似分解：**

![20250801-184200.png](.\马尔可夫img\20250801-184200.png)

![20250801-185238.png](.\马尔可夫img\20250801-185238.png)



![20250801-185928.png](.\马尔可夫img\20250801-185928.png)

![20250801-190832.png](.\马尔可夫img\20250801-190832.png)

## 回溯图

## 最优状态价值函数

**是所有策略产生的状态价值函数中，使状态s价值最大打函数：**

![20250801-191834.png](.\马尔可夫img\20250801-191834.png)

最优价值函数可能有多个

![20250801-193814.png)](.\马尔可夫img\20250801-193814.png)





## 最优动作价值函数

**所有策略产生的动作价值函数中，使状态-行为（s,a）对价值最大的函数：**

![20250801-193504.png](.\马尔可夫img\20250801-193504.png)

同样，最优动作价值函数也有可能有多个

**最优价值函数明确了MDP的最优可能表现**

**一旦最优价值函数知晓，则认为MDP已完成求解**

## 最优策略

![20250801-194414.png](.\马尔可夫img\20250801-194414.png)

**最优策略：**在有限状态和动作的MDP中，一定有一个策略又于其他策略，这就是最优策略

![20250801-194545.png](.\马尔可夫img\20250801-194545.png)

![20250801-194045.png](.\马尔可夫img\20250801-194045.png)

解释：策略，动作可能有多个，但是最优价值函数只会有一个

**寻找最优策略：**

![20250801-194749.png](.\马尔可夫img\20250801-194749.png)



![20250801-195218.png](.\马尔可夫img\20250801-195218.png)

## 贝尔曼最优方程

![20250801-195503.png](.\马尔可夫img\20250801-195503.png)

![20250801-195702.png](.\马尔可夫img\20250801-195702.png)

![20250801-195739.png](.\马尔可夫img\20250801-195739.png)

![20250801-200042.png](.\马尔可夫img\20250801-200042.png)



![20250801-200345.png](.\马尔可夫img\20250801-200345.png)

# 笔记说明

## ** 参考：**

### 1.北京邮电大学 **鲁鹏** **强化学习基础**

### 2.中国工信出版社 张伟楠，沈键，俞勇   动手学强化学习

